# Ollama API Configuration
# URL where Ollama is running (default: http://localhost:11434)
OLLAMA_API_URL=http://localhost:11434

# Model name to use for LLM operations (default: llama3:8b)
# Examples: llama3:8b, gemma:7b, gemma:2b, mistral:7b
OLLAMA_CHAT_MODEL=llama3:8b
